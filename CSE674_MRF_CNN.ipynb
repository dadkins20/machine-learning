{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSE674 MRF-CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIjjZgGvq5LR",
        "colab_type": "code",
        "outputId": "6580649c-045f-412e-f5b9-74d7e140b4c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        }
      },
      "source": [
        "!pip install torch==1.4.0 torchvision==0.5.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4MB)\n",
            "\u001b[K     |█████▉                          | 138.3MB 1.5MB/s eta 0:06:39"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qG2iE9YnrAnW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import torchvision.models as models\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fK6mOa_mZ2qE",
        "colab_type": "code",
        "outputId": "612728d5-e752-4b97-b694-8e7803503620",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# device = xm.xla_device()\n",
        "print(device)\n",
        "\n",
        "# import my Google Drive for saved models\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_path = '/content/drive/My Drive/Colab Notebooks/mrf-cnn'\n",
        "content_path = base_path + '/content-images'\n",
        "style_path = base_path + '/style-images'\n",
        "output_path = base_path + '/output'"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6kl7ZkJxIE-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# based on https://github.com/jonzhaocn/cnnmrf-pytorch/blob/master/mylibs.py but logic is my own.\n",
        "\n",
        "# The Content Loss class just does an MSE loss against the content image (Formula 4 from paper)\n",
        "class ContentLoss(nn.Module):\n",
        "    def __init__(self, target):\n",
        "        super(ContentLoss, self).__init__()\n",
        "        self.target = target.detach()\n",
        "        self.loss = None\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.loss = F.mse_loss(input, self.target)\n",
        "        return input\n",
        "\n",
        "    def update(self, target):\n",
        "        self.target = target.detach()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlSVaFnLxPUq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# based on https://github.com/jonzhaocn/cnnmrf-pytorch/blob/master/mylibs.py\n",
        "\n",
        "# The Style Loss class handles the MRF loss function against the style image (Formula 2,3 from paper)\n",
        "class StyleLoss(nn.Module):\n",
        "    def __init__(self, target):\n",
        "        super(StyleLoss, self).__init__()\n",
        "        self.patch_size = (3,3)\n",
        "        self.stride = 1 #set to 2 because 1 crashed system.\n",
        "        self.gpu_chunk_size = 512\n",
        "        self.loss = None\n",
        "\n",
        "        # this can be pre-computed as it doesn't change during the forward pass\n",
        "        self.style_patches = self.patches_sampling(target.detach(), patch_size=self.patch_size, stride=self.stride)\n",
        "        self.style_patches_norm = self.cal_patches_norm()\n",
        "        self.style_patches_norm = self.style_patches_norm.view(-1, 1, 1)\n",
        "\n",
        "    def update(self, target):\n",
        "        # we do however have to update it when the image resolution changes\n",
        "        self.style_patches = self.patches_sampling(target.detach(), patch_size=self.patch_size,stride=self.stride)\n",
        "        self.style_patches_norm = self.cal_patches_norm()\n",
        "        self.style_patches_norm = self.style_patches_norm.view(-1, 1, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        synthesis_patches = self.patches_sampling(input, patch_size=self.patch_size, stride=self.stride)\n",
        "        max_response = []\n",
        "        for i in range(0, self.style_patches.shape[0], self.gpu_chunk_size):\n",
        "            i_start = i\n",
        "            i_end = min(i+self.gpu_chunk_size, self.style_patches.shape[0])\n",
        "            weight = self.style_patches[i_start:i_end, :, :, :]\n",
        "            response = F.conv2d(input, weight, stride=self.stride)\n",
        "            max_response.append(response.squeeze(dim=0))\n",
        "        max_response = torch.cat(max_response, dim=0)\n",
        "\n",
        "        max_response = max_response.div(self.style_patches_norm)\n",
        "        max_response = torch.argmax(max_response, dim=0)\n",
        "        max_response = torch.reshape(max_response, (1, -1)).squeeze()\n",
        "        # loss\n",
        "        loss = 0\n",
        "        for i in range(0, len(max_response), self.gpu_chunk_size):\n",
        "            i_start = i\n",
        "            i_end = min(i+self.gpu_chunk_size, len(max_response))\n",
        "            tp_ind = tuple(range(i_start, i_end))\n",
        "            sp_ind = max_response[i_start:i_end]\n",
        "            loss += torch.sum(torch.mean(torch.pow(synthesis_patches[tp_ind, :, :, :]-self.style_patches[sp_ind, :, :, :], 2), dim=[1, 2, 3]))\n",
        "        self.loss = loss/len(max_response)\n",
        "        return input\n",
        "\n",
        "    def patches_sampling(self, image, patch_size, stride):\n",
        "        h, w = image.shape[2:4]\n",
        "        patches = []\n",
        "        for i in range(0, h - patch_size[0] + 1, stride):\n",
        "            for j in range(0, w - patch_size[1] + 1, stride):\n",
        "                patches.append(image[:, :, i:i + patch_size[0], j:j + patch_size[1]])\n",
        "        patches = torch.cat(patches, dim=0).to(device)\n",
        "        return patches\n",
        "\n",
        "    def cal_patches_norm(self):\n",
        "        # norm of style image patches\n",
        "        norm_array = torch.zeros(self.style_patches.shape[0])\n",
        "        for i in range(self.style_patches.shape[0]):\n",
        "            norm_array[i] = torch.pow(torch.sum(torch.pow(self.style_patches[i], 2)), 0.5)\n",
        "        return norm_array.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1ggVbbtxU2U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is the regularizer from the paper.  It's the squared gradient norm (Formula 5 from paper) which is used to smooth the image\n",
        "class Regularizer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Regularizer, self).__init__()\n",
        "        self.loss = None\n",
        "        self.unnormalize = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "\n",
        "    def forward(self, input):\n",
        "        image = self.unnormalize(input.clone().squeeze()).permute([1, 2, 0])\n",
        "        xij_1 = torch.cat((image[1:, :, :], image[-1, :, :].unsqueeze(0)), dim=0)\n",
        "        xij_1 = xij_1 - image\n",
        "        xi_1j = torch.cat((image[:, 1:, :], image[:, -1, :].unsqueeze(1)), dim=1)\n",
        "        xi_1j = xi_1j - image\n",
        "\n",
        "        self.loss = torch.sum(torch.pow(xij_1, 2) + torch.pow(xi_1j, 2))\n",
        "        return input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3i3yqgcUrEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MRF_CNN(nn.Module):\n",
        "    def __init__(self, style_image, content_image):\n",
        "        super(MRF_CNN, self).__init__()\n",
        "        self.alpha1 = .5\n",
        "        self.alpha2 = 0.01\n",
        "        self.style_layers = [13, 22] # 13 is relu3_1, 22 is relu4_1 as per https://www.mathworks.com/help/deeplearning/ref/vgg19.html\n",
        "        self.content_layers = [24] # 24 is relu4_2\n",
        "\n",
        "        # build the model based on vgg19 and insert the custom content and style loss layers\n",
        "        vgg = models.vgg19(pretrained=True).to(device)\n",
        "        model = nn.Sequential()\n",
        "        content_losses = []\n",
        "        style_losses = []\n",
        "        regularizer = Regularizer()\n",
        "        model.add_module('regularizer', regularizer)\n",
        "\n",
        "        for i in range(len(vgg.features)):\n",
        "            # add layer of vgg19\n",
        "            layer = vgg.features[i]\n",
        "            name = str(i)\n",
        "            model.add_module(name, layer)\n",
        "\n",
        "            # add content loss layer\n",
        "            if i in self.content_layers:\n",
        "                target = model(content_image).detach()\n",
        "                content_loss = ContentLoss(target)\n",
        "                model.add_module(\"content_loss_\" + name, content_loss)\n",
        "                content_losses.append(content_loss)\n",
        "\n",
        "            # add style loss layer\n",
        "            if i in self.style_layers:\n",
        "                target_feature = model(style_image).detach()\n",
        "                style_loss = StyleLoss(target_feature)\n",
        "                model.add_module(\"style_loss_\" + name, style_loss)\n",
        "                style_losses.append(style_loss)\n",
        "\n",
        "        self.model = model\n",
        "        self.content_losses = content_losses\n",
        "        self.style_losses = style_losses\n",
        "        self.regularizer = regularizer\n",
        "\n",
        "    def forward(self, image):\n",
        "        self.model(image)\n",
        "        style_loss = 0\n",
        "        content_loss = 0\n",
        "\n",
        "        # calculate losses\n",
        "        for x in self.style_losses:\n",
        "            style_loss += x.loss\n",
        "        for x in self.content_losses:\n",
        "            content_loss += x.loss\n",
        "        loss = style_loss + (self.alpha1 * content_loss) + (self.alpha2 * self.regularizer.loss)\n",
        "        return loss\n",
        "\n",
        "    def update(self, style_image, content_image):\n",
        "        # update the target of style loss layer\n",
        "        x = style_image.clone()\n",
        "        next_style_idx = 0\n",
        "        i = 0\n",
        "        for layer in self.model:\n",
        "            if isinstance(layer, Regularizer) or isinstance(layer, ContentLoss) or isinstance(layer, StyleLoss):\n",
        "                continue\n",
        "            x = layer(x)\n",
        "            if i in self.style_layers:\n",
        "                # extract feature of style image in vgg19 as style loss target\n",
        "                self.style_losses[next_style_idx].update(x)\n",
        "                next_style_idx += 1\n",
        "            i += 1\n",
        "\n",
        "        # update the target of content loss layer\n",
        "        x = content_image.clone()\n",
        "        next_content_idx = 0\n",
        "        i = 0\n",
        "        for layer in self.model:\n",
        "            if isinstance(layer, Regularizer) or isinstance(layer, ContentLoss) or isinstance(layer, StyleLoss):\n",
        "                continue\n",
        "            x = layer(x)\n",
        "            if i in self.content_layers:\n",
        "                # extract feature of content image in vgg19 as content loss target\n",
        "                self.content_losses[next_content_idx].update(x)\n",
        "                next_content_idx += 1\n",
        "            i += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX0jUs2_bRq2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from https://discuss.pytorch.org/t/simple-way-to-inverse-transform-normalization/4821/3\n",
        "class UnNormalize(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
        "        Returns:\n",
        "            Tensor: Normalized image.\n",
        "        \"\"\"\n",
        "        for t, m, s in zip(tensor, self.mean, self.std):\n",
        "            t.mul_(s).add_(m)\n",
        "            # The normalize code -> t.sub_(m).div_(s)\n",
        "        return tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RkZ0048Vsnc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this function finds all the images in content and style path and then encodes them\n",
        "def get_and_transform_images(content_path, style_path):\n",
        "\n",
        "    # make sure paths exist first and they contain files\n",
        "    if not os.path.isdir(content_path):\n",
        "        raise ValueError('directory %s does not exist.' % content_path)\n",
        "    if not os.path.isdir(style_path):\n",
        "        raise ValueError('directory %s does not exist.' % style_path)\n",
        "    if len(os.listdir(content_path) ) == 0:\n",
        "        raise ValueError('directory %s is empty.' % content_path)\n",
        "    if len(os.listdir(style_path) ) == 0: \n",
        "        raise ValueError('directory %s is empty.' % style_path)\n",
        "\n",
        "    # Pretrained models must have images at least 3x224x224 in size and must be \n",
        "    # normalized by mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225].\n",
        "    # Source: https://pytorch.org/docs/stable/torchvision/models.html\n",
        "    transform = T.Compose([T.ToTensor(),T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n",
        "\n",
        "    # read all the images in the folder and then convert to RGB and apply the transform\n",
        "    content_images = []\n",
        "    for cimg in os.listdir(content_path):\n",
        "      if os.path.isdir(cimg):\n",
        "        continue\n",
        "      print(cimg)\n",
        "      img = cv2.imread(content_path + \"/\" + cimg)\n",
        "      img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "      img = transform(img).unsqueeze(0).to(device)\n",
        "      content_images.append(img)\n",
        "\n",
        "    style_images = []\n",
        "    for simg in os.listdir(style_path):\n",
        "      if os.path.isdir(simg):\n",
        "        continue\n",
        "      print(simg)\n",
        "      img = cv2.imread(style_path + \"/\" + simg)\n",
        "      img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "      img = transform(img).unsqueeze(0).to(device)\n",
        "      style_images.append(img)\n",
        "\n",
        "    # need to build an image pyramid using scaling factor of two and stop when longest dimension is less than 64 pixels\n",
        "    images_pyramid_content = []\n",
        "    for img in content_images:\n",
        "      newimg = img\n",
        "      img_pyramid = []\n",
        "      img_pyramid.append(newimg)\n",
        "      images_pyramid_content.append(img_pyramid)\n",
        "\n",
        "    images_pyramid_style = []\n",
        "    for img in style_images:\n",
        "      newimg = img\n",
        "      img_pyramid = []\n",
        "      img_pyramid.append(newimg)\n",
        "      images_pyramid_style.append(img_pyramid)\n",
        "\n",
        "    return images_pyramid_content, images_pyramid_style"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcWru7GFLO76",
        "colab_type": "code",
        "outputId": "fcea9255-9260-4494-ed57-d0252d870e5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# the main function that runs the algorithm\n",
        "\n",
        "global iteration\n",
        "max_iterations = 200\n",
        "content_images, style_images = get_and_transform_images(content_path, style_path)\n",
        "\n",
        "unnormalize = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "\n",
        "for x, content_image in enumerate(content_images):\n",
        "  for y, style_image in enumerate(style_images):\n",
        "\n",
        "    timer = float(time.time())\n",
        "\n",
        "    iteration = 0\n",
        "    synthesized_image = None\n",
        "\n",
        "    # create the model\n",
        "    model = MRF_CNN(style_image=style_image[0], content_image=content_image[0]).to(device)\n",
        "    model.train()\n",
        "\n",
        "    synthesized_image = content_image[0].clone().requires_grad_(True).to(device)\n",
        "\n",
        "    def closure():\n",
        "      global iteration\n",
        "      optimizer.zero_grad()\n",
        "      loss = model(synthesized_image)\n",
        "      loss.backward()\n",
        "\n",
        "      # save image every after each set of iterations\n",
        "      if (iteration + 1) % 50 == 0:\n",
        "          image = unnormalize(synthesized_image.clone().squeeze())\n",
        "          image = F.interpolate(image.unsqueeze(0), size=content_image[0].shape[2:4], mode='bilinear', align_corners=True)\n",
        "          torchvision.utils.save_image(image.squeeze(), output_path + '/image-c%d-s%d-it%d.jpg' % (x, y, iteration + 1))\n",
        "          print('save image: image-c%d-s%d-it%d.jpg loss: %f' % (x, y, iteration + 1, loss.item()))\n",
        "\n",
        "      iteration += 1\n",
        "\n",
        "      if iteration == max_iterations:\n",
        "          iteration = 0\n",
        "      return loss\n",
        "\n",
        "    optimizer = optim.LBFGS([synthesized_image], max_iter=max_iterations)\n",
        "    optimizer.step(closure)\n",
        "\n",
        "    print(\"Execution Time: %6.3f\" % (float(time.time()) - timer))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "content.jpg\n",
            "style.jpg\n",
            "save image: image-c0-s0-it50.jpg loss: 28.839828\n",
            "save image: image-c0-s0-it100.jpg loss: 26.996178\n",
            "save image: image-c0-s0-it150.jpg loss: 25.985758\n",
            "save image: image-c0-s0-it200.jpg loss: 25.251431\n",
            "Execution Time: 332.495\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dx4w0sPQnTcz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}